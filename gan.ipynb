{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from gpt import (\n",
    "    create_file_search_assistant,\n",
    "    upload_file_to_vectore_store,\n",
    "    assistant_use_vector_store,\n",
    "    create_thread,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o\"\n",
    "FILE = \"gan.pdf\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_file_search_assistant took 0.35389184951782227 seconds\n"
     ]
    }
   ],
   "source": [
    "assistant = create_file_search_assistant(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    assistant_name=\"Machine learning Assistant\",\n",
    "    instructions=\"You are an machine learning expert. Use you knowledge base to extract and summarize information about audited machine learning papers\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n",
      "upload_file_to_vectore_store took 3.4466092586517334 seconds\n"
     ]
    }
   ],
   "source": [
    "vector_store = upload_file_to_vectore_store(\n",
    "    client=client,\n",
    "    vector_store_name=\"Machine learning papers\",\n",
    "    file_paths=[FILE],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant_use_vector_store took 0.6176433563232422 seconds\n"
     ]
    }
   ],
   "source": [
    "assistant = assistant_use_vector_store(\n",
    "    client=client,\n",
    "    assistant=assistant,\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "create_thread took 0.3602719306945801 seconds\n"
     ]
    }
   ],
   "source": [
    "thread = create_thread(\n",
    "    client=client,\n",
    "    message_content=\"Make detailed summary of the paper and explain the maths formulas\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Detailed Summary of the Paper on Generative Adversarial Networks (GANs)\n",
      "\n",
      "#### 1. Introduction\n",
      "The paper introduces a novel framework called Generative Adversarial Networks (GANs) for estimating generative models via an adversarial process. This process simultaneously trains two models: a generative model (G) to capture the data distribution and a discriminative model (D) to estimate the probability of a sample coming from the training data rather than G .\n",
      "\n",
      "#### 2. Adversarial Framework\n",
      "The adversarial model framework involves a game between G and D. The generator transforms noise \\( z \\) (sampled from a distribution \\( p_z(z) \\)) into data space using a function \\( G(z; \\theta_g) \\) represented by multilayer perceptrons. The discriminator \\( D(x; \\theta_d) \\), also a multilayer perceptron, distinguishes between real data and generated data by outputting a probability .\n",
      "\n",
      "The objective is for \\( D \\) to maximize its success in distinguishing between real and fake data, while \\( G \\) aims to maximize the probability of \\( D \\) making a mistake. This is formulated as a minimax two-player game:\n",
      "\n",
      "\\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] \\]\n",
      "\n",
      "In an ideal training scenario, \\( G \\) and \\( D \\) reach a point where \\( D \\) cannot distinguish between real and generated data any better than random guessing, meaning \\( D(x) = \\frac{1}{2} \\) everywhere  .\n",
      "\n",
      "### 3. Theoretical Background\n",
      "The generator \\( G \\) implicitly defines a probability distribution \\( p_g \\) through the samples \\( G(z) \\). The paper theoretically proves that the minimax game reaches a global optimum where \\( p_g = p_{data} \\), ensuring that the generative model perfectly replicates the data distribution .\n",
      "\n",
      "**Optimal Discriminator:** For a fixed generator \\( G \\):\n",
      "\\[ D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \\]\n",
      "\n",
      "The training criterion for the discriminator \\( D \\), given \\( G \\), is to maximize:\n",
      "\\[ V(G, D) = \\int_x p_{data}(x) \\log D(x) + p_g(x) \\log (1 - D(x)) \\]\n",
      "\n",
      "**Global Minimum:** The paper proves that the global minimum of the virtual training criterion \\( C(G) \\) is achieved if and only if \\( p_g = p_{data} \\), at which point \\( C(G) = -\\log 4 \\)  .\n",
      "\n",
      "### 4. Training Algorithm\n",
      "The training alternates between updating the discriminator \\( D \\) to distinguish real data from fake, and updating the generator \\( G \\) to generate data that \\( D \\) cannot distinguish from real data. This process is outlined in Algorithm 1:\n",
      "\n",
      "1. Update \\( D \\) by ascending its stochastic gradient.\n",
      "2. Update \\( G \\) by descending its stochastic gradient.\n",
      "\n",
      "The paper suggests that \\( G \\) can be trained to maximize \\( \\log D(G(z)) \\) instead of minimizing \\( \\log (1 - D(G(z))) \\) to avoid gradient saturation early in learning  .\n",
      "\n",
      "### 5. Experiments and Results\n",
      "Experiments were conducted on datasets like MNIST, the Toronto Face Database (TFD), and CIFAR-10. The results showed that adversarial networks can generate competitive samples compared to existing methods. The generator network used a mix of rectifier linear and sigmoid activations, while the discriminator used maxout activations  .\n",
      "\n",
      "### 6. Advantages and Disadvantages\n",
      "**Advantages:**\n",
      "- Markov chains are not required.\n",
      "- Training utilizes only backpropagation, with no need for inference.\n",
      "- Can represent very sharp distributions without the need for approximation methods like variational inference.\n",
      "\n",
      "**Disadvantages:**\n",
      "- There is no explicit representation of \\( p_g(x) \\).\n",
      "- \\( D \\) and \\( G \\) must be well-synchronized to avoid the \"Helvetica scenario\" where \\( G \\) collapses too many values to a single value, reducing diversity  .\n",
      "\n",
      "### Math Formulas Explanation\n",
      "1. **Training Objective:**\n",
      "\\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))] \\]\n",
      "\n",
      "2. **Optimal Discriminator \\( D^*_G(x) \\):**\n",
      "\\[ D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \\]\n",
      "\n",
      "3. **Virtual Training Criterion \\( C(G) \\):**\n",
      "\\[ \n",
      "C(G) = \\mathbb{E}_{x \\sim p_{data}} \\left[ \\log \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \\right] + \\mathbb{E}_{x \\sim p_g} \\left[ \\log \\frac{p_g(x)}{p_{data}(x) + p_g(x)} \\right] \n",
      "\\]\n",
      "\\[ \n",
      "C(G) = -\\log(4) + 2 \\cdot \\text{JSD}(p_{data} \\parallel p_g) \n",
      "\\]\n",
      "\n",
      "The formulas capture the core mechanism of GANs, where the generator and discriminator are pitted in a zero-sum game, ultimately driving the generator to produce data indistinguishable from the real distribution.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"=\"*50)\n",
    "print(\"\\n\".join(citations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
